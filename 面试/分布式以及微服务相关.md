# ****分布式相关****

### 分布式事务

#### 2PC

```
				2PC是基于XA协议的两阶段提交协议（Two- Phase Commit Protocol）

				预提交阶段(Pre-Commit Phase)

					两阶段提交在应用程序向协调者发出一个提交命令时被启动。这时提交进入第一阶段，即预提交阶段 .协调者准备局部（即在本地）提交并在日志中写入"预提交"日志项，并包含有该事务的所有参与者的名字 ,如果全部都提交答复并且不否定则进入到第二个提交阶段，如果其中有一个参与者不能参与答复，或者否定了一个，那么就会取消事务，撤销他们的影响

				决策后阶段（Post-Decision Phase）

					如果所有的参与者都送回"已准备好提交"的消息，则该事务的提交进入第二阶段，即决策后提交阶段，协调者在日志中写入"提交"日志项，并立即写入硬盘 ，然后送回"已提交"的消息，释放该事务占用的资源。  如果所有都回复已提交那么就完成事务的提交，如果提交发生异常则回滚

			优点：

				 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。 

			缺点：

				实现复杂，牺牲了可用性，对性能影响较大，涉及多次节点间的网络通信，通信时间太长，不适合高并发高性能场景 
```

#### 3PC

```

```

#### TCC

```
				TCC（Try Confirm Cancel） 其实就是采用的补偿机制，

					Try

						这里是对业务系统做检测及资源预留

					Confirm

						在 Confirm 阶段，执行远程操作，如果成功则直接成功，如果失败则执行Cancel

					Cancel

						主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放

				 优点：实现简单，方便

				 缺点：数据的一致性差，并且需要写非常多的补偿代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理 
```

#### AT

​		Auto Transaction,AT模式根据其名称也能反馈出来他的特性，他是自动型的分布式事务解决方案。这个自动提现在他无需代码入侵，也就是说我们不需要再编写多余的代码来实现这个模式，只需要在方法中添加上指定的注解即可。

#### MQ 事务消息

```
	 有一些第三方的 MQ 是支持事务消息的，比如 RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交

		第一阶段 Prepared 消息，会拿到消息的地址。

		第二阶段执行本地事务

		第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。 

	，	一次发送消息和一次确认消息。如果确认消息发送失败了 RocketMQ 会定期扫描消息集群中的事务消息，这时候发现了 Prepared 消息，它会向消息发送者确认，所以生产方需要实现一个 check 接口，RocketMQ 会根据发送端设置的策略来决定是回滚还是继续发送确认消息。 

		优点： 实现了最终一致性，不需要依赖本地数据库事务。

		缺点： 实现难度大，主流MQ不支持，没有.NET客户端，RocketMQ事务消息部分代码也未开源。
```

### 分布式锁如何设计

```
		分布式锁的设计有三种

			数据库
				
			Redis

				使用setnx并且设置超时时间，只有一个人能拿到锁，拿到了锁就直接进行数据库查询然后将数据添加到Redis当中，然后没有拿到锁的先睡眠一秒（防止还是没有数据，一秒钟可以将数据存入缓存），然后调用自己的方法回去，现在再次查询redis已经是有数据的了，这样就实现了分布式查询锁

			Zookeeper
```

### CAP以及Base理论

#### CAP定理

			分布式的不可能全部满足三个需求
	
				一致性（C：Consistency）
	
				可用性（A：Availability）
	
				分区容错性（P：Partition tolerance） 
	
				CA  
	
					放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择 
	
				CP
	
					放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多NoSQL系统就是如此 
	
				AP
	
					放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用 
	
		分布式事务可以使用多个方案

Base(Basically Available，Soft state,Eventually consistent)：一种 ACID的替代方案 

	Basically Available（基本可用）
	
		基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性----注意，这绝不等价于系统不可用 ，比如响应时间慢了1秒或者下订单失败进入失败页面，其实这就是一个强一致性
	
	Soft state（软状态）
	
		软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时 ，也就是数据同步时间不一致，也称弱一致性
	
	Eventually consistent（最终一致性）
	
		最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。当这个程序执行完成之后所有的数据都一致，这个是最终一致性
#### Base理论





### 分布式session如何设计

```
	分布式session的实现方式有很多种

			基于数据库的session共享

			基于Redis的session缓存

			基于cookie的session共享

		这里我们推荐使用Redis，因为数据库性能并不高，而且我们也需要减少数据库压力，而cookie则容易被获取，从而造成数据的安全隐患，所以使用Redis既能优化速度，也能保证安全，还能方便清理
```

### dubbo的组件有哪些

```
		Remoting: 网络通信框架，实现了 sync-over-async 和request-response 消息机制. 		

		RPC: 一个远程过程调用的抽象，支持负载均衡、容灾和集群功能
		
		Registry: 服务目录框架用于服务的注册和服务事件发布和订阅
```



### dubbo是如何利用接口就可以通信的

那我们就先来从dubbo的在通信中使用到的数据吧，

​	**Protocol：**注册协议，包括zookeeper、multicast、Redis、simple； 

​	Address：**注册地址，dubbo服务的IP+Port：

​					   ①、当使用zk，address填入zk地址，集群地址使用","分隔；

​					   ②、使用dubbo直连，address填写直连地址和服务端口；

​	**Protocol：**使用的dubbo协议，包括dubbo、rmi、hessian、webservice、memcached、redis，根据自己的协议类型选择对应的选项即可； 

​	**Timeout：**请求超时时间，单位ms，根据dubbo具体配置填写； 

​	Version：**版本，dubbo不同版本之间差异较大，不同版本之间不能互相调用，这里指定dubbo版本，是为了方便识别和说明； 

​	**Retries：**异常重试次数（类似这种分布式服务通信框架，大多都有重试机制，是为了保证事务成功率）； 

​	**Cluster：**集群类型，包括failover、failfast、failsafe、failback、failking； 

​	**Group：**组类型，如果有的话，根据配置填写即可； 

​	**Connections：**连接数，同上，根据配置填写； 

​	**Async：**服务处理类型，包括sync（同步）、async（异步），根据配置填写； 

​	**Loadbalance：**负载均衡策略，包括random（随机）、roundrobin（轮询）、leastactive（最少活跃数）、consistenthash（一致性哈希）； 

​	**Interface：**接口名（因为dubbo服务大多是开发根据规范自行命名的，因此这里需要填写完整的接口名+包名）； 

​	**Method：**当前接口下的方法名，按照开发提供的API文档填写即可； 

​	**Args：**接口报文，根据API文档填写，如上图所示，添加输入行，输入对应的参数类型和值即可（参数类型和值如何定义填写，请参考上面的链接）； 

​	①、paramType：参数支持任何类型，包装类直接使用`java.lang`下的包装类，小类型使用：`int``、``float``、``shot``、``double``、``long``、``byte``、``boolean``、``char`，自定义类使用类完全名称； 

​	②、paramValue：基础包装类和基础小类型直接使用值，例如：int为1，boolean为true等，自定义类与`List`或者`Map`等使用json格式数据； 

​	这里我们可以看到他记录了非常多的东西，甚至ip以及协议，接口名，方法名还有参数，那么他回去进行匹配zookeeper的节点，如果匹配了就通过ip和接口还有方法名进行调用，他的调用的过程是不经过zookeeper的，因为我们拿到了ip和接口就可以直接进行rpc的远程调用，甚至与如果zookeeper挂掉了，其实他们地也是有缓存的还能继续使用，从这点可以看出并没有经过zookeeper，rpc的远程调用使用的原理是socket，那么他在通信是已经拿到了ip那么就可以直接调用他的服务了，这就是dubbo利用通信的原理

### Zookeeper在选举的过程中，还能对外提供服务么？

​		这个就需要结合情况看了，如果是单纯的使用zookeeper的话那肯定是对外提供不了服务了，但是如果是使用的dubbo的话那就是还能使用的，因为dubbo是将服务的注册信息都写入到了zookeeper的节点中，如果他开始就通过zookeeper调用过的话本地是有缓存的，当zookeeper在选举的过程中，如果他还是调用访问过的服务那就是没问题的，除非缓存过期了。

# 微服务相关

### Ribbon的默认负载均衡算法有几种

```
RoundRobinRule			     轮询
```

```
RandomRule			         随机

AvailabilityFilteringRule	  会先过滤多次访问故障而处于断路器跳闸状态的服务

WeightedResponseTimeRule 	  根据平均响应时间计算所有的服务权重，响应时间越快服务权重越大就越容易被选中，刚启动时信息统计不足，会先使用轮询，等到响应数据有了之后在进行响应时间进行均衡负载
```

```
RetryRule 					先按照轮询的方式，如果获取到服务失败等就会在指定的时间内进行重试，然后获取下一个可用的服务

BastAvailableRule			 会先过滤掉由于多次访问故障而处于断路器跳闸的服务，然后选择一个并发量最小的服务

ZoneAvoidanceRule		     默认规则，符合判断服务所在的区域的性能和服务的可用性选择服务器
```

### Eureka和Zookeeper的区别

著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性) 。

Zookeeper保证CP



​	所以Zookeeper的一致性比较强，但是他的可用性并不高，体现在如果一个Zookeeper集群挂掉了父节点，那么他会重新进行选举，而在这个选举的过程中他是无法提供服务的



Eureka保证AP

​	

​	在Eureka的集群中，他保证了可用性，也就是当一个注册中心挂掉了之后他会带着服务去另一个注册中心，这个并不影响他们的可用性，只要还有一口气他的服务就能提供，除非所有的注册中心都挂掉了

### Eureka的运行流程和实现原理

Register（服务注册）	 ----》 	Renew（服务续约   	----》  	 Cancel（服务下线） 

服务注册一般是发生在服务启动的时候，后面如果服务自身检测认为 Down，也会来更新服务状态的。 

服务续约，主要是用来告诉 Eureka Server Service Provider 还活着，避免服务被剔除掉。 

服务下线一般在 Service Provider shut down 的时候调用，用来把自身的服务从 Eureka Server 中删除，以防客户端调用不存在的服务。 

小结：服务的注册表示服务发现，将自己的信息注册到Eureka上面，并且不定期的发送检测机制（心跳检测），如果有相应就进行服务续约，如果发现一个服务响应超时或者说访问不了，那么他就会将服务剔除掉，这就是服务下线



实现原理：

​		其实主要还是使用Servlet实现的，它使用 Jersey ，框架实现自身的 RESTful HTTP接口，peer之间的同步与服务的注册全部通过 HTTP 协议实现 ，他的心跳机制等都是通过定时任务去进行实现的



### Eureka的核心

#### 心跳检测机制

什么是心跳检测机制？

​	就是当你的服务再运行的过程中他会时不时地去检测你当前的服务是否健康如果响应超时或者访问不到它就会进行服务剔除，默认是90秒，这个是可以修改的，但是如果Eureka和服务之间出现了网络故障本来就是由于自己的网络状态所引起的那么他就会进入到自我保护状态

#### 自我保护机制

​	自我保护机制的工作机制是如果在15分钟内超过85%的客户端节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，Eureka Server自动进入自我保护机制，此时会出现以下几种情况： 



1、Eureka Server不再从注册列表中移除因为长时间没收到心跳而应该过期的服务。



2、Eureka Server仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上，保证当前节点依然可用。



3、当网络稳定时，当前Eureka Server新的注册信息会被同步到其它节点中。



这里我们可以看到哪怕他是出现了故障的时候他都会保证当前节点的可用性。以及自己的容错性，所以在个Zookeeper的对比中他做到了AP(可用性和分区容错性)

#### Eureka服务端配置

```properties
#心跳检测，毫秒，改为10秒，如果10秒没有收到服务的信息，那么就会将服务剔除掉，默认60*1000,60秒
eureka.server.eviction-interval-timer-in-ms=10000

#是否开启自我保护，默认true
eureka.server.enable-self-preservation=true

#是否将自己注册到注册中心上以供其他服务发现，默认true
eureka.client.register-with-eureka=false

#是否从注册中心上，获取注册表的信息
eureka.client.fetch-registry=false

```

#### Eureka客户端配置

```properties
#eureka客户端需要多长时间发送心跳给eureka服务器，表明他仍然或者，默认30秒
eureka.instance.lease-renewal-interval-in-seconds=30

#eureka服务器在接受到实力的最后一次发出的心跳后，需要等待多久才可以将此实例删除，默认90秒
eureka.instance.lease-expiration-duration-in-seconds=30

#表示eureka client间隔多久去拉取服务器注册信息,默认为30秒
eureka.client.registry-fetch-interval-seconds=18

#表示eureka的注册中心的地址
eureka.client.serviceUrl.defaultZone=http://127.0.0.1:8081/eureka/
```

### Feign客户端负载均衡以及熔断

#### Feign客户端的实现原理

​    Feign 的英文表意为“假装，伪装，变形”， 是一个http请求调用的轻量级框架，可以以Java接口注解的方式调用Http请求，而不用像Java中通过封装HTTP请求报文的方式直接调用。 他是基于HttpClient进行调用的，

### Nacos动态配置原理

​		首先我们找到自动装配

```java
@Configuration(proxyBeanMethods = false)
// 自动配置，默认开启
@ConditionalOnProperty(name = "spring.cloud.nacos.config.enabled", matchIfMissing = true)
public class NacosConfigBootstrapConfiguration {
  
   // 获取配置里面的NacosConfig属性配置，SpringBoot配置文件
   @Bean
   @ConditionalOnMissingBean
   public NacosConfigProperties nacosConfigProperties() {
      return new NacosConfigProperties();
   }
   
   // NacosConfig配置管理器（核心）
   @Bean
   @ConditionalOnMissingBean
   public NacosConfigManager nacosConfigManager(
         NacosConfigProperties nacosConfigProperties) {
      return new NacosConfigManager(nacosConfigProperties);
   }
    
   // 第二步注册一个NacosPropertySourceLocator
   @Bean
   public NacosPropertySourceLocator nacosPropertySourceLocator(
         NacosConfigManager nacosConfigManager) {
      return new NacosPropertySourceLocator(nacosConfigManager);
   }

}
```

​		NacosConfigManager

```java
static ConfigService createConfigService(
      NacosConfigProperties nacosConfigProperties) {
   // 检查是否初始化service，双重检查
   if (Objects.isNull(service)) {
      synchronized (NacosConfigManager.class) {
         try {
            if (Objects.isNull(service)) {
               // 初始化（核心）
               service = NacosFactory.createConfigService(
                     nacosConfigProperties.assembleConfigServiceProperties());
            }
         }
         catch (NacosException e) {
            log.error(e.getMessage());
            throw new NacosConnectionFailureException(
                  nacosConfigProperties.getServerAddr(), e.getMessage(), e);
         }
      }
   }
   return service;
}
```

​		NacosConfigService

```java
public NacosConfigService(Properties properties) throws NacosException {
    String encodeTmp = properties.getProperty(PropertyKeyConst.ENCODE);
    if (StringUtils.isBlank(encodeTmp)) {
        encode = Constants.ENCODE;
    } else {
        encode = encodeTmp.trim();
    }
    // 初始化命名空间
    initNamespace(properties);
    // http请求的代理，用于请求Nacos，会循环检查登录状态重新登录
    agent = new MetricsHttpAgent(new ServerHttpAgent(properties));
    agent.start();
  	// 客户端工作对象（核心）
    worker = new ClientWorker(agent, configFilterChainManager);
}
```

​		ClientWorker

```java
public ClientWorker(final HttpAgent agent, final ConfigFilterChainManager configFilterChainManager) {
    this.agent = agent;
    this.configFilterChainManager = configFilterChainManager;

  	// 初始化调度器线程，核心线程1，无界队列
    executor = Executors.newScheduledThreadPool(1, new ThreadFactory() {
        @Override
        public Thread newThread(Runnable r) {
            Thread t = new Thread(r);
            t.setName("com.alibaba.nacos.client.Worker." + agent.getName());
            t.setDaemon(true);
            return t;
        }
    });

  	// 初始化长轮询线程池，核心线程1，无界队列
    executorService = Executors.newCachedThreadPool(new ThreadFactory() {
        @Override
        public Thread newThread(Runnable r) {
            Thread t = new Thread(r);
            t.setName("com.alibaba.nacos.client.Worker.longPolling" + agent.getName());
            t.setDaemon(true);
            return t;
        }
    });

  	// 延时调度任务，初始延迟1ms，10ms一次，检查配置详情，每过10ms检查一次配置信息是否更变
    executor.scheduleWithFixedDelay(new Runnable() {
        public void run() {
            try {
              	// 核心
                checkConfigInfo();
            } catch (Throwable e) {
                LOGGER.error("[" + agent.getName() + "] [sub-check] rotate check error", e);
            }
        }
    }, 1L, 10L, TimeUnit.MILLISECONDS);
}
```

​		checkConfigInfo()

```java
public void checkConfigInfo() {
    // 监听的数量
    int listenerSize = cacheMap.get().size();
  	// 长轮询的任务数量
    int longingTaskCount = (int)Math.ceil(listenerSize / ParamUtil.getPerTaskConfigSize());
    // 如果长轮询的任务数量，大于当前的任务数量则执行任务，一直到和长轮询任务数量相等
    if (longingTaskCount > currentLongingTaskCount) {
        for (int i = (int)currentLongingTaskCount; i < longingTaskCount; i++) {
          	// 执行长轮询任务
            executorService.execute(new LongPollingRunnable(i));
        }
        currentLongingTaskCount = longingTaskCount;
    }
}
```

​		LongPollingRunnable

```java
class LongPollingRunnable implements Runnable {
    private int taskId;

    public LongPollingRunnable(int taskId) {
        this.taskId = taskId;
    }

    public void run() {
        try {
            List<CacheData> cacheDatas = new ArrayList<CacheData>();
            // check failover config
            // 检查本地的配置文件，用于故障切换等情况
            for (CacheData cacheData : cacheMap.get().values()) {
                if (cacheData.getTaskId() == taskId) {
                    cacheDatas.add(cacheData);
                    try {
                        checkLocalConfig(cacheData);
                        if (cacheData.isUseLocalConfigInfo()) {
                            cacheData.checkListenerMd5();
                        }
                    } catch (Exception e) {
                        LOGGER.error("get local config info error", e);
                    }
                }
            }
                        
            List<String> inInitializingCacheList = new ArrayList<String>();
            // check server config
            // 检查服务端的配置文件，获取更变的数据
            List<String> changedGroupKeys = checkUpdateDataIds(cacheDatas, inInitializingCacheList);
            for (String groupKey : changedGroupKeys) {
                String[] key = GroupKey.parseKey(groupKey);
                String dataId = key[0];
                String group = key[1];
                String tenant = null;
                if (key.length == 3) {
                    tenant = key[2];
                }
                try {
                    String content = getServerConfig(dataId, group, tenant, 3000L);
                    CacheData cache = cacheMap.get().get(GroupKey.getKeyTenant(dataId, group, tenant));
                    cache.setContent(content);
                    LOGGER.info("[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}",
                        agent.getName(), dataId, group, tenant, cache.getMd5(),
                        ContentUtils.truncateContent(content));
                } catch (NacosException ioe) {
                    String message = String.format(
                        "[%s] [get-update] get changed config exception. dataId=%s, group=%s, tenant=%s",
                        agent.getName(), dataId, group, tenant);
                    LOGGER.error(message, ioe);
                }
            }
            // 检查是否有更变，校验MD5
            for (CacheData cacheData : cacheDatas) {
                if (!cacheData.isInitializing() || inInitializingCacheList
                    .contains(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant))) {
                    // （核心）
                    cacheData.checkListenerMd5();
                    cacheData.setInitializing(false);
                }
            }
            inInitializingCacheList.clear();
        } catch (Throwable e) {
            LOGGER.error("longPolling error", e);
        } finally {
            // 继续调用自己，轮询。
            executorService.execute(this);
        }
    }
}
```

​		checkListenerMd5()

```java
void checkListenerMd5() {
        // 轮询所有监听器，并且校验md5，如果不一样则通知监听器
    for (ManagerListenerWrap wrap : listeners) {
        if (!md5.equals(wrap.lastCallMd5)) {
            safeNotifyListener(dataId, group, content, md5, wrap);
        }
    }
}
```

​		safeNotifyListener

```java
private void safeNotifyListener(final String dataId, final String group, final String content,
                                final String md5, final ManagerListenerWrap listenerWrap) {
    final Listener listener = listenerWrap.listener;

    Runnable job = new Runnable() {
        public void run() {
            ClassLoader myClassLoader = Thread.currentThread().getContextClassLoader();
            ClassLoader appClassLoader = listener.getClass().getClassLoader();
            try {
                if (listener instanceof AbstractSharedListener) {
                    AbstractSharedListener adapter = (AbstractSharedListener)listener;
                    adapter.fillContext(dataId, group);
                    LOGGER.info("[{}] [notify-context] dataId={}, group={}, md5={}", name, dataId, group, md5);
                }
                // 执行回调之前先将线程classloader设置为具体webapp的classloader，以免回调方法中调用spi接口是出现异常或错用（多应用部署才会有该问题）。
                Thread.currentThread().setContextClassLoader(appClassLoader);

                ConfigResponse cr = new ConfigResponse();
                cr.setDataId(dataId);
                cr.setGroup(group);
                cr.setContent(content);
                configFilterChainManager.doFilter(null, cr);
                String contentTmp = cr.getContent();
                // 监听器接收配置信息
                listener.receiveConfigInfo(contentTmp);
                listenerWrap.lastCallMd5 = md5;
                LOGGER.info("[{}] [notify-ok] dataId={}, group={}, md5={}, listener={} ", name, dataId, group, md5,
                    listener);
            } catch (NacosException de) {
                LOGGER.error("[{}] [notify-error] dataId={}, group={}, md5={}, listener={} errCode={} errMsg={}", name,
                    dataId, group, md5, listener, de.getErrCode(), de.getErrMsg());
            } catch (Throwable t) {
                LOGGER.error("[{}] [notify-error] dataId={}, group={}, md5={}, listener={} tx={}", name, dataId, group,
                    md5, listener, t.getCause());
            } finally {
                Thread.currentThread().setContextClassLoader(myClassLoader);
            }
        }
    };
    final long startNotify = System.currentTimeMillis();
        try {
            if (null != listener.getExecutor()) {
                listener.getExecutor().execute(job);
            } else {
                job.run();
            }
        } catch (Throwable t) {
            LOGGER.error("[{}] [notify-error] dataId={}, group={}, md5={}, listener={} throwable={}", name, dataId, group,
                md5, listener, t.getCause());
        }
        final long finishNotify = System.currentTimeMillis();
        LOGGER.info("[{}] [notify-listener] time cost={}ms in ClientWorker, dataId={}, group={}, md5={}, listener={} ",
            name, (finishNotify - startNotify), dataId, group, md5, listener);
    }
```

​		调用相关listener的receiveConfigInfo()方法，最终调用innerReceive()方法

​		innerReceive()方法里发送真正的RefreshEvent事件，通知cloud去刷新配置。

​		这个配置被RefreshEventListener监听到，然后刷新环境内的值。这个有兴趣可以去这个类里跟一下源码。



### Nacos服务发现原理

